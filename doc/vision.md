# 1. Технологии

Для реализации MVP Telegram-бота выбран следующий стек:

* **Язык программирования:** Python 3.11+
* **Telegram API:** `python-telegram-bot`
* **LLM-провайдер:** OpenRouter через OpenAI client (используется для генерации конспектов и перевода текста на русский язык)
* **Парсинг PDF:** `pdfplumber` или `PyPDF2` для извлечения текста из PDF
* **Хранение данных:** только в памяти (`dict`/`list`), без баз данных
* **Тестирование:** `pytest`
* **Окружение:** `uv`
* **Автоматизация:** `Makefile`
* **Деплой:** `Docker`
---

# 2. Принципы разработки

- **KISS (Keep It Simple, Stupid):** минимальная сложность решений, использование только необходимых инструментов
- **MVP-first:** сначала реализуем минимально жизнеспособный продукт (работа с текстом и PDF, генерация 2 видов конспекта)  
- **Итеративная разработка:** маленькие шаги → быстрые проверки → улучшения
- **Прозрачность:** простая и понятная архитектура, без скрытых зависимостей
- **Легко поддерживать:** читаемый код, простая модульная структура 
- **Минимум инфраструктуры:** без баз данных и сторонних сервисов, всё хранится в памяти
---

# 3. Структура проекта 
```
llmstart-homework/
├── bot/                              
│   ├── __init__.py
│   ├── main.py                       → запуск бота
│   ├── handlers.py                   → обработка команд и сообщений
│   ├── keyboards.py                  → кнопки Telegram
│   ├── parser.py                     → приём данных от пользователя и передача в сервисы
│   ├── response_formatter.py         → подготовка и форматирование ответов пользователю
│   └── utils.py                      → вспомогательные функции для бота
│
├── llm/                 
│   ├── __init__.py
│   ├── client.py                     → обёртка для работы с API LLM 
│   ├── summarizer.py                 → генерация конспектов (структурированный/для новичка)
│   └── translator.py                 → перевод текста на русский
│
├── services/                         
│   ├── __init__.py
│   ├── pdf_service.py                → извлечение текста из PDF
│   ├── text_service.py               → очистка, нормализация и разбиение текста на чанки
│   └── summary_service.py            → объединение пайплайна: PDF → перевод → конспект
│
├── tests/               
│   ├── __init__.py
│   ├── test_bot.py                   → тесты логики Telegram (handlers, keyboards, parser)
│   ├── test_llm.py                   → тесты LLM (client, summarizer, translator) с моками
│   └── test_services.py              → тесты пайплайнов и PDF (pdf_service, text_service, summary_service)
│
├── doc/               
│   ├── product_idea.md   
│   ├── vision.md       
│   ├── conventions.md   
│   ├── tasklist.md   
│   └── workflow.md        
│
├── pyproject.toml       
├── Makefile             
├── Dockerfile            
├── .env                  
└── README.md            
```
---

# 4. Архитектура проекта

Архитектура Telegram-бота построена с разделением ответственности по слоям:  
- **Интерфейс:** `bot/`  
- **Обработка текста:** `services/`  
- **Работа с LLM:** `llm/`  

1. **Инициализация и запуск бота**  
   - Настройка бота (`bot/main.py`)  
   - Регистрация команд и обработчиков сообщений (`bot/main.py`)  
   - Передача всех событий в `handlers.py` (`bot/main.py`)

2. **Приветствие и выбор формата конспекта**  
   - Отправка приветственного сообщения пользователю (`bot/handlers.py`)  
   - Показ кнопок для выбора формата конспекта (`bot/keyboards.py`)  
   - Логика обработки выбора формата (`bot/handlers.py`)  

3. **Приём и проверка данных от пользователя**  
   - Получение текста, PDF или ссылки (`bot/handlers.py`)  
   - Проверка формата и размера данных (`bot/parser.py`)  
   - Определение потока обработки: PDF → `services/pdf_service.py`, текст → `services/text_service.py` (`bot/parser.py`)  
   - Вспомогательные проверки и дополнительный chunking (`bot/utils.py`)  
   - Обработка ошибок: некорректный формат или слишком большой файл → уведомление пользователя (`bot/parser.py`, `bot/handlers.py`)  

4. **Извлечение текста из PDF**  
   - Парсинг PDF, очистка артефактов (`services/pdf_service.py`)  
   - Передача чистого текста в `text_service.py` (`services/pdf_service.py`)  
   - Обработка ошибок: если текст не удалось извлечь → уведомление пользователя (`services/pdf_service.py`)  

5. **Обработка текста и подготовка чанков**  
   - Очистка и нормализация текста (`services/text_service.py`)  
   - Разбиение на чанки (~3000–4000 символов) (`services/text_service.py`)  
   - Обработка ошибок: пустой или слишком короткий текст → уведомление пользователя (`services/text_service.py`) 

6. **Перевод текста на русский при необходимости**  
   - Проверка языка и перевод чанков (`llm/translator.py`)  
   - Передача переведённых чанков в `summarizer.py` (`services/summary_service.py`)  
   - Обработка ошибок: сбой перевода → уведомление пользователя (`llm/translator.py`)  

7. **Генерация конспекта**  
   - Формирование конспекта для каждого чанка (`llm/summarizer.py`)  
   - Агрегация всех чанков в итоговый конспект (`services/summary_service.py`)  
   - Управление API-запросами к LLM (`llm/client.py`)  
   - Обработка ошибок: сбой LLM → уведомление пользователя (`llm/client.py`, `llm/summarizer.py`)  

8. **Форматирование и подготовка ответа для Telegram**  
   - Разбиение конспекта на части (~4096 символов) (`bot/response_formatter.py`)  
   - Форматирование заголовков, списков, переносов строк (`bot/response_formatter.py`)  
   - Вспомогательные проверки и утилиты (`bot/utils.py`)  

9. **Отправка конспекта пользователю**  
   - Отправка сообщений через Telegram API (`bot/response_formatter.py`, `bot/handlers.py`)  
   - Поддержка выбранного формата конспекта  
   - Обработка ошибок: сбой отправки → уведомление пользователя (`bot/handlers.py`)


### Ключевые особенности архитектуры

- **Chunking для больших документов**: PDF делится на части, чтобы весь текст учитывался
- **Многошаговая работа с LLM**: сначала частичные конспекты, затем итоговое объединение
- **Обработка ошибок**: бот сообщает пользователю про ошибки
---

# 5. Работа с LLM
В проекте для генерации конспектов и перевода текста используется LLM через **OpenRouter с OpenAI client**.  Взаимодействие с моделью организовано через модуль `llm/client.py`, а логика конспектирования и перевода реализована в `llm/summarizer.py` и `llm/translator.py`.  

## Архитектура работы с LLM

1. **Подготовка данных**  
   - Текст разбивается на чанки (~3000–4000 символов) в `services/text_service.py`
   - При необходимости чанки переводятся на русский через `llm/translator.py`

2. **Генерация конспекта**  
   - Для каждого чанка создаётся частичный конспект (`llm/summarizer.py`).  
   - Итоговый конспект собирается в `services/summary_service.py`

3. **Обработка ошибок**  
   - Если LLM не отвечает или возвращает ошибку, бот уведомляет пользователя (`llm/client.py`, `llm/summarizer.py`)
   - Ошибки логируются для последующего анализа

## Промпты для LLM

Промпты задают модели, как именно нужно формировать ответы. Они хранятся в соответствующих модулях (`summarizer.py` и `translator.py`) и передаются вместе с чанком текста.

**Промпт для структурированного конспекта**
```
Ты - помощник для создания учебных конспектов. Создай структурированный конспект по следующим разделам:
- Основные идеи и тезисы
- Ключевые понятия и определения
- Важные примеры и case studies
- Практическое применение
- Выводы и рекомендации
```

**Промпт для объяснения "для новичка"**
```
Ты - преподаватель, который объясняет сложные темы начинающим. Создай простое объяснение:
- Объясни как для 5-летнего ребенка
- Используй аналогии и метафоры
- Приведи простые примеры из жизни
- Выдели самое важное без технического жаргона
```

**Промпт для перевода**
```
Ты - профессиональный переводчик технических текстов. Переведи текст на русский язык, сохраняя:
- Техническую точность терминов
- Структуру и логику изложения
- Естественность русского языка
```
---

# 6. Мониторинг LLM

Мониторинг LLM в проекте направлен на контроль стабильности работы модели, обработку ошибок и анализ качества сгенерированных конспектов. Все действия сосредоточены вокруг модулей `llm/client.py`, `llm/summarizer.py` и `llm/translator.py`.

## Основные задачи мониторинга

1. **Отслеживание ошибок API**  
   - Проверка ответа от LLM через `llm/client.py`.  
   - Логирование ошибок сети, тайм-аутов и некорректных ответов.  
   - Уведомление бота о сбоях для информирования пользователя (`bot/handlers.py`).

2. **Логирование запросов и ответов**  
   - Все запросы к LLM сохраняются с метаданными: размер чанка, тип промпта, язык текста.  
   - Ответы модели логируются для последующего анализа качества конспекта и перевода.  

3. **Отслеживание времени отклика**  
   - Измеряется время обработки каждого чанка.  
   - При превышении порогового значения фиксируется предупреждение.  

4. **Качество генерации**  
   - Ведётся контроль на стороне бота: если текст с ошибками или пустой, бот уведомляет пользователя и логирует событие.  
   - Позволяет выявлять ухудшение качества работы LLM или проблемы с промптами.  

5. **Аварийная обработка**  
   - При сбое LLM (отсутствие ответа, ошибка API) генерация останавливается.  
   - Пользователь получает уведомление о проблеме.  
---

# 7. Сценарий работы Telegram-бота

Ниже представлен пошаговый сценарий работы бота с пользователем, включая все ключевые модули и логику обработки.

1. **Старт бота**  
   - Пользователь запускает бота командой `/start`.  
   - `bot/main.py` инициализирует Telegram API и регистрирует обработчики.  

2. **Приветствие и выбор формата конспекта**  
   - `bot/handlers.py` отправляет приветственное сообщение.  
   - Пользователь видит кнопки из `bot/keyboards.py` и выбирает формат:
     - Конспект по разделам  
     - Объяснение «для новичка»  

3. **Отправка данных пользователем**  
   - Пользователь отправляет текст, PDF или ссылку на PDF.  
   - `bot/handlers.py` и `bot/parser.py` проверяют тип и размер данных.  
   - Вспомогательные проверки и дополнительный chunking выполняются через `bot/utils.py`.  
   - Ошибки (например, неподдерживаемый формат или слишком большой файл) обрабатываются сразу с уведомлением пользователя.

4. **Извлечение текста из PDF (если применимо)**  
   - `services/pdf_service.py` извлекает текст из PDF через `pdfplumber`/`PyPDF2`.  
   - Текст очищается от лишнего форматирования.  
   - Если извлечение не удалось, бот уведомляет пользователя.

5. **Обработка текста и подготовка чанков**  
   - `services/text_service.py` нормализует текст и делит его на чанки (~3000–4000 символов).  
   - Ошибки, например пустой или слишком короткий текст, обрабатываются с уведомлением пользователя.

6. **Перевод текста на русский (при необходимости)**  
   - `llm/translator.py` проверяет язык каждого чанка и при необходимости переводит на русский.  
   - Переведённые чанки передаются в `llm/summarizer.py`.  
   - Сбои перевода логируются и пользователю отправляется уведомление.

7. **Генерация конспекта**  
   - `llm/summarizer.py` создаёт частичные конспекты для каждого чанка, используя промпты для выбранного формата.  
   - `services/summary_service.py` объединяет все чанки в итоговый конспект.  
   - Ошибки LLM (отсутствие ответа, некорректные данные) фиксируются и пользователю отправляется уведомление.

8. **Подготовка ответа для Telegram**  
   - `bot/response_formatter.py` разбивает итоговый конспект на сообщения (~4096 символов) и форматирует текст.  
   - Мелкие проверки и утилиты выполняются через `bot/utils.py`.

9. **Отправка результата пользователю**  
   - `bot/response_formatter.py` и `bot/handlers.py` отправляют готовый конспект пользователю.  
   - Пользователь получает текст в выбранном формате (структурированный или для новичка).  
   - В случае ошибки отправки бот уведомляет пользователя.

---


# 8. Деплой
Проект развёртывается через Docker для удобства и воспроизводимости окружения.

- **Dockerfile**: 
  - Устанавливает Python 3.11  
  - Устанавливает зависимости из `pyproject.toml`  
  - Копирует код проекта  
  - Запускает бота через `python -m bot.main`  

- **Переменные окружения**: 
  - Хранятся в `.env`  
  - Содержат `TELEGRAM_TOKEN`, `OPENROUTER_API_KEY` и другие настройки  

- **Запуск контейнера**:
```
docker build -t llmstart-bot .
docker run --env-file .env -p 8080:8080 llmstart-bot
```

- **Makefile** позволяет автоматизировать сборку и запуск Docker-контейнера:

```
# Сборка контейнера
make build
# Запуск контейнера
make run
```
---

# 9. Подход к конфигурированию

Конфигурация проекта вынесена в отдельные файлы и переменные окружения для удобства настройки и безопасности.

- **Файл `.env`**:
  - Хранит ключи API и другие чувствительные данные:
    - `TELEGRAM_TOKEN` — токен Telegram-бота  
    - `OPENROUTER_API_KEY` — ключ доступа к LLM через OpenRouter  
    - Другие настройки (например, лимиты на размер обрабатываемого текста)  
  - Используется библиотекой `python-dotenv` для загрузки переменных окружения в код.

- **Конфигурация Python-проектов**:
  - `pyproject.toml` содержит зависимости и настройки сборки.
  - Настройки бота, LLM и сервисов можно вынести в отдельный модуль `config.py` для централизованного доступа:
    ```python
    import os

    TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    MAX_CHUNK_SIZE = int(os.getenv("MAX_CHUNK_SIZE", 4000))
    ```

- **Подход**:
  - Все ключи и параметры, которые могут меняться между окружениями (dev, staging, prod), находятся в `.env`.
  - Код остаётся неизменным при смене окружения — достаточно обновить `.env`.
  - Чувствительные данные не хранятся в репозитории.
  - Значения по умолчанию задаются в `config.py` или через переменные окружения.

---

# 10. Подход к логированию

Логирование используется для отслеживания работы бота, мониторинга запросов к LLM и выявления ошибок в процессе обработки данных.

- **Цель логирования**:
  - Отслеживать события пользователя (запросы, отправка файлов, выбор формата конспекта)
  - Мониторить работу LLM (запросы, ответы, ошибки)
  - Логировать ошибки в обработке PDF и текста
  - Обеспечить возможность последующего анализа и отладки

- **Используемые инструменты**:
  - Стандартная библиотека Python: `logging`
  - Настройка уровня логов (`DEBUG`, `INFO`, `WARNING`, `ERROR`)
  - Возможность логирования в файл или в консоль
  - Формат сообщений включает timestamp, уровень, модуль и сообщение
